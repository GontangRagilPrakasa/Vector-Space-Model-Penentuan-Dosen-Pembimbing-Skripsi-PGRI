{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Library\n",
    "#preprocessing library\n",
    "import pandas as pd\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory \n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords # preprocessing\n",
    "from nltk.stem import PorterStemmer # preprocessing bahasa inggris\n",
    "import re\n",
    "#TF IDF Library\n",
    "from sklearn.feature_extraction.text import CountVectorizer # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer # tf-idf\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "#CLSA Library\n",
    "from scipy.linalg import svd \n",
    "from numpy import dot\n",
    "from scipy.linalg import svd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kalimat = input()\n",
    "\n",
    "#Ikatan Dokter Anak Indonesia ( IDAI) memberikan sejumlah rekomendasi untuk mencegah penularan Covid-19 kepada anak di masa pandemi. Salah satu yang direkomendasikan adalah agar anak tidak keluar rumah selama situasi Covid-19 di Indonesia belum memenuhi kriteria epidemiologi badan kesehatan dunia (WHO). Kami merekomendasikan agar anak-anak jangan keluar rumah dulu. Termasuk untuk kegiatan tatap muka di sekolah, ujar anggota Tim Satgas Penanganan Covid-19 IDAI, Yogi Prawira. Rekomendasi untuk tidak keluar rumah ini, berlaku hingga daerah tempat tinggal anak-anak dianggap sudah dapat mengatasi penularan Covid-19 lewat transmisi lokal. Namun, kata Yogi, rekomendasi ini dikecualikan jika ada keperluan mendesak yang membuat anak untuk keluar rumah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "def splitParagraphIntoSentences(paragraph):\n",
    "    sentenceEnders = re.compile('[.!?]')\n",
    "    sentenceList = sentenceEnders.split(paragraph)\n",
    "    return sentenceList\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()  # Object stemmer\n",
    "remover = StopWordRemoverFactory().create_stop_word_remover()  # objek stopword\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def stemmerEN(text):\n",
    "    porter = PorterStemmer()\n",
    "    ## stop = set(stopwords.words('english')) #stopwods berguna untuk\n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopwords = factory.get_stop_words()\n",
    "    text = text.lower()\n",
    "    text = [i for i in text.lower().split() if i not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    preprocessed_text = text.translate(translator)\n",
    "    text_stem = porter.stem(preprocessed_text)\n",
    "    return text_stem\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower() #cashfolding\n",
    "    text_clean = remover.remove(text) #filtering\n",
    "    text_stem = stemmer.stem(text_clean) #stemming\n",
    "    text_stem = stemmerEN(text_stem) #Proses Preprocessing EN\n",
    "    return text_stem\n",
    "\n",
    "\n",
    "pemisal_kalimat = splitParagraphIntoSentences(kalimat) #melempar paramter kalimat\n",
    "# print(\"kalimat asli\")\n",
    "# print(kalimat)\n",
    "# print(\"kalimat setelah \")\n",
    "# print(pemisal_kalimat)\n",
    "# print(\"kalimat text clean\")\n",
    "simpan_sementara_isi_berita = list()\n",
    "berita_asli = list()\n",
    "for per_kalimat in pemisal_kalimat:\n",
    "    simpan_sementara_isi_berita.append(preprocessing(per_kalimat.strip()))\n",
    "    berita_asli.append(per_kalimat.strip())\n",
    "    \n",
    "#tampikan hasil dari pemisahan titik dan preprocessing\n",
    "print(simpan_sementara_isi_berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-eadd1f2d6a66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#norm=None norm-l2 (not + 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimpan_sementara_isi_berita\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#proses pencarian TF IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mreturn_TFIDF\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_TFIDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \"\"\"\n\u001b[0;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords,smooth_idf=False, norm=None) #norm=None norm-l2 (not + 1)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(simpan_sementara_isi_berita) #proses pencarian TF IDF\n",
    "return_TFIDF  = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names()).T\n",
    "print(return_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ini adalah fungsi global untuk pemanggilan Kalimat yang sudah di hasilkan berdasarkan summary CLSA.LSA\n",
    "def summary_sentence(doc, angka, result_TFIDF, types=''):\n",
    "    types = types.lower()\n",
    "    angkas = int(angka)\n",
    "    # try :\n",
    "    if (types == 'lsa') :\n",
    "        matrixAT=result_TFIDF.toarray()\n",
    "        matrixA=np.transpose(matrixAT)\n",
    "        matrixAT=np.transpose(matrixA)\n",
    "        a = np.array(matrixAT)\n",
    "        b = np.array(matrixA)\n",
    "        aTa = dot(a,b)\n",
    "        tempSVD = np.array(aTa)\n",
    "        u, s, v = svd(tempSVD)\n",
    "        s = sp.diag(s)\n",
    "        datas = list()\n",
    "        loop = v[0]\n",
    "        if(angkas <= len(s[0])):\n",
    "            for x in range(0, 8):\n",
    "                print(x)\n",
    "                loop = s[0]\n",
    "                temp2 = 0\n",
    "                temp = list()\n",
    "                for i in np.arange(np.size(loop)):\n",
    "                    for j in np.arange(np.size(loop)):\n",
    "                        temp2 = temp2 + np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "                    if(temp2 == temp[x]):\n",
    "                        if(temp2 != 0):\n",
    "                            print(doc[i])\n",
    "                    temp2 = 0\n",
    "        else:\n",
    "            print('Jumlah input melebihi jumlah dokumen')\n",
    "        return datas\n",
    "            \n",
    "    elif (types == 'clsa') :\n",
    "        matrixAT=result_TFIDF.toarray()\n",
    "        matrixA=np.transpose(matrixAT)\n",
    "        matrixAT=np.transpose(matrixA)\n",
    "        a = np.array(matrixAT)\n",
    "        b = np.array(matrixA)\n",
    "        aTa = dot(a,b)\n",
    "        tempSVD = np.array(aTa)\n",
    "        u, s, v = svd(tempSVD)\n",
    "        s = sp.diag(s)\n",
    "        aa =v[0]\n",
    "        for i in np.arange(np.size(aa)):\n",
    "            av = np.average(v[i])\n",
    "            for j in np.arange(np.size(aa)):\n",
    "                if v[i][j] < av :\n",
    "                    v[i][j] = 0\n",
    "\n",
    "        loop = s[0]\n",
    "        temp = 0\n",
    "        data = list()\n",
    "        for i in np.arange(np.size(loop)):\n",
    "            for j in np.arange(np.size(loop)):\n",
    "                temp = temp + np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "            if(temp != 0):\n",
    "                data.append(doc[i])\n",
    "            temp = 0\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4bede9ff0e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mreturn_CLSA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCLSA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0msentences_clsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mberita_asli\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'clsa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "#kemudian kita cari CLSA dari hasil TF-IDF\n",
    "\n",
    "def CLSA(tfidf):\n",
    "    matrixAT=tfidf.toarray()\n",
    "    matrixA=numpy.transpose(matrixAT)\n",
    "    matrixAT=numpy.transpose(matrixA)\n",
    "    a = np.array(matrixAT)\n",
    "    b = np.array(matrixA)\n",
    "    aTa = dot(a,b)\n",
    "    A = np.array(aTa)\n",
    "    u, s, v = np.linalg.svd(A, full_matrices = False)\n",
    "    s = sp.diag(s)\n",
    "    aa =v[0]\n",
    "    for i in np.arange(np.size(aa)):\n",
    "        av = np.average(v[i])\n",
    "        for j in np.arange(np.size(aa)):\n",
    "            if v[i][j] < av :\n",
    "                v[i][j] = 0\n",
    "    loop = s[0]\n",
    "    for i in np.arange(np.size(loop)):\n",
    "        for j in np.arange(np.size(loop)):\n",
    "            s[i][j] = np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "    return s\n",
    "\n",
    "def sum_frame_by_column(frame, new_col_name, list_of_cols_to_sum):\n",
    "    frame[new_col_name] = frame[list_of_cols_to_sum].astype(float).sum(axis=1)\n",
    "    return(frame)\n",
    "\n",
    "a = int(input())\n",
    "return_CLSA = pd.DataFrame(CLSA(X)).T\n",
    "sentences_clsa = summary_sentence(berita_asli,a, X, types='clsa')\n",
    "rank_CLSA = sum_frame_by_column(return_CLSA, 'total_score_document', [i for i in range(len(return_CLSA[0]))])\n",
    "soritng = sorted(rank_CLSA['total_score_document'], reverse=True)\n",
    "# print(return_CLSA)\n",
    "print(sentences_clsa)\n",
    "# print(rank_CLSA)\n",
    "# print(soritng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kemudian kita banding kan dengan metode LSA\n",
    "def LSA(tfidf): \n",
    "    matrixAT=tfidf.toarray()\n",
    "    matrixA=np.transpose(matrixAT)\n",
    "    matrixAT=np.transpose(matrixA)\n",
    "    a = np.array(matrixAT)\n",
    "    b = np.array(matrixA)\n",
    "    aTa = dot(a,b)\n",
    "    tempSVD = np.array(aTa)\n",
    "    u, s, v = np.linalg.svd(tempSVD, full_matrices = False)\n",
    "    s = sp.diag(s)\n",
    "    loop = s[0]\n",
    "    for i in np.arange(np.size(loop)):\n",
    "        for j in np.arange(np.size(loop)):\n",
    "            s[i][j] = np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "    return s\n",
    "\n",
    "a = int(input())\n",
    "return_LSA = pd.DataFrame(LSA(X)).T\n",
    "sentences_lsa = summary_sentence(berita_asli,a, X, types='lsa')\n",
    "\n",
    "print(return_LSA)\n",
    "print(sentences_lsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
